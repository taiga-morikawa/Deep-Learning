# 定番ライブラリ
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# 実行時間計測ライブラリ
import time

# 標準化ライブラリ
from sklearn.preprocessing import StandardScaler

data = pd.read_csv("dirty_data.csv")
data

name	height	weight
0	A	160cm	49kg
1	B	170cm	37kg
2	C	1.81m	80
3	D	NaN	50kg
4	E	1.90m	80
5	F	170cm	NaN
6	G	150cm	40

# height列の処理
for i in range(len(data)):
    if pd.isna(data["height"][i]):
        continue
        
    if "cm" in data["height"][i]:
        data["height"][i] = float(data["height"][i][:-2])
    elif "m" in data["height"][i]:
        data["height"][i] = float(data["height"][i][:-1])
        data["height"][i] *= 100
        
data

	name	height	weight
0	A	160	49kg
1	B	170	37kg
2	C	181	80
3	D	NaN	50kg
4	E	190	80
5	F	170	NaN
6	G	150	40

# weight列の処理
for i in range(len(data)):
    if pd.isna(data["weight"][i]):
        continue
        
    if "kg" in data["weight"][i]:
        data["weight"][i] = float(data["weight"][i][:-2])
        
data

	name	height	weight
0	A	160	49
1	B	170	37
2	C	181	80
3	D	NaN	50
4	E	190	80
5	F	170	NaN
6	G	150	40

# for文は非常に遅いことの確認
long_data = pd.DataFrame(np.array(range(10000)), columns=["test"])
long_data

	test
0	0
1	1
2	2
3	3
4	4
5	5
6	6
7	7
8	8
9	9
10	10
11	11
12	12
13	13
14	14
15	15
16	16
17	17
18	18
19	19
20	20
21	21
22	22
23	23
24	24
25	25
26	26
27	27
28	28
29	29
...	...
9970	9970
9971	9971
9972	9972
9973	9973
9974	9974
9975	9975
9976	9976
9977	9977
9978	9978
9979	9979
9980	9980
9981	9981
9982	9982
9983	9983
9984	9984
9985	9985
9986	9986
9987	9987
9988	9988
9989	9989
9990	9990
9991	9991
9992	9992
9993	9993
9994	9994
9995	9995
9996	9996
9997	9997
9998	9998
9999	9999
10000 rows × 1 columns

# 処理開始時刻
start_time = time.time()

for i in range(len(long_data)):
    long_data["test"][i] += 1

# 処理終了時刻
end_time = time.time()
    
display(long_data)
print(end_time - start_time)

	test
0	1
1	2
2	3
3	4
4	5
5	6
6	7
7	8
8	9
9	10
10	11
11	12
12	13
13	14
14	15
15	16
16	17
17	18
18	19
19	20
20	21
21	22
22	23
23	24
24	25
25	26
26	27
27	28
28	29
29	30
...	...
9970	9971
9971	9972
9972	9973
9973	9974
9974	9975
9975	9976
9976	9977
9977	9978
9978	9979
9979	9980
9980	9981
9981	9982
9982	9983
9983	9984
9984	9985
9985	9986
9986	9987
9987	9988
9988	9989
9989	9990
9990	9991
9991	9992
9992	9993
9993	9994
9994	9995
9995	9996
9996	9997
9997	9998
9998	9999
9999	10000
10000 rows × 1 columns

0.5366878509521484

# ある関数をすべての要素に適用するapply関数を使うと速い

# データ読み込み直し
long_data = pd.DataFrame(np.array(range(10000)), columns=["test"])

# 下記の.apply(add1)を実行時、add1(x)のxに入るのはlong_data["test"]の各要素
def add1(x):
    return x+1

# 処理開始時刻
start_time = time.time()

long_data["test"] = long_data["test"].apply(add1)

# 処理終了時刻
end_time = time.time()
    
display(long_data)
print(end_time - start_time)

	test
0	1
1	2
2	3
3	4
4	5
5	6
6	7
7	8
8	9
9	10
10	11
11	12
12	13
13	14
14	15
15	16
16	17
17	18
18	19
19	20
20	21
21	22
22	23
23	24
24	25
25	26
26	27
27	28
28	29
29	30
...	...
9970	9971
9971	9972
9972	9973
9973	9974
9974	9975
9975	9976
9976	9977
9977	9978
9978	9979
9979	9980
9980	9981
9981	9982
9982	9983
9983	9984
9984	9985
9985	9986
9986	9987
9987	9988
9988	9989
9989	9990
9990	9991
9991	9992
9992	9993
9993	9994
9994	9995
9995	9996
9996	9997
9997	9998
9998	9999
9999	10000

10000 rows × 1 columns

0.003245830535888672

# 先ほどのheightとweghtの処理をapplyで書き直し
data = pd.read_csv("dirty_data.csv")

def height_to_num(height):
    if pd.isna(height):
        return height
    if "cm" in height:
        return float(height[:-2])
    if "m" in height:
        return float(height[:-1]) * 100

def weight_to_num(weight):
    if pd.isna(weight):
        return weight
    if "kg" in weight:
        return float(weight[:-2])
    else:
        return float(weight)
    
data["height"] = data["height"].apply(height_to_num)
data["weight"] = data["weight"].apply(weight_to_num)

data

	name	height	weight
0	A	160.0	49.0
1	B	170.0	37.0
2	C	181.0	80.0
3	D	NaN	50.0
4	E	190.0	80.0
5	F	170.0	NaN
6	G	150.0	40.0

# 欠損値除去
data.dropna()

	name	height	weight
0	A	160.0	49.0
1	B	170.0	37.0
2	C	181.0	80.0
4	E	190.0	80.0
6	G	150.0	40.0

# 前後の平均で埋める
data.interpolate()

	name	height	weight
0	A	160.0	49.0
1	B	170.0	37.0
2	C	181.0	80.0
3	D	185.5	50.0
4	E	190.0	80.0
5	F	170.0	60.0
6	G	150.0	40.0

# name列をダミー化する
pd.get_dummies(data)

	height	weight	name_A	name_B	name_C	name_D	name_E	name_F	name_G
0	160.0	49.0	1	0	0	0	0	0	0
1	170.0	37.0	0	1	0	0	0	0	0
2	181.0	80.0	0	0	1	0	0	0	0
3	NaN	50.0	0	0	0	1	0	0	0
4	190.0	80.0	0	0	0	0	1	0	0
5	170.0	NaN	0	0	0	0	0	1	0
6	150.0	40.0	0	0	0	0	0	0	1

# name列をダミー化する（最初のカテゴリ除去）
data = pd.get_dummies(data, drop_first=True)
data

	height	weight	name_B	name_C	name_D	name_E	name_F	name_G
0	160.0	49.0	0	0	0	0	0	0
1	170.0	37.0	1	0	0	0	0	0
2	181.0	80.0	0	1	0	0	0	0
3	NaN	50.0	0	0	1	0	0	0
4	190.0	80.0	0	0	0	1	0	0
5	170.0	NaN	0	0	0	0	1	0
6	150.0	40.0	0	0	0	0	0	1

# 標準化
data = data.interpolate()
StandardScaler().fit_transform(data)

/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.
  return self.partial_fit(X, y)
/anaconda3/lib/python3.7/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.
  return self.fit(X, **fit_params).transform(X)
array([[-0.93512147, -0.46352336, -0.40824829, -0.40824829, -0.40824829,
        -0.40824829, -0.40824829, -0.40824829],
       [-0.17837577, -1.19816417,  2.44948974, -0.40824829, -0.40824829,
        -0.40824829, -0.40824829, -0.40824829],
       [ 0.6540445 ,  1.43429871, -0.40824829,  2.44948974, -0.40824829,
        -0.40824829, -0.40824829, -0.40824829],
       [ 0.99458006, -0.4023033 , -0.40824829, -0.40824829,  2.44948974,
        -0.40824829, -0.40824829, -0.40824829],
       [ 1.33511563,  1.43429871, -0.40824829, -0.40824829, -0.40824829,
         2.44948974, -0.40824829, -0.40824829],
       [-0.17837577,  0.20989737, -0.40824829, -0.40824829, -0.40824829,
        -0.40824829,  2.44948974, -0.40824829],
       [-1.69186718, -1.01450397, -0.40824829, -0.40824829, -0.40824829,
        -0.40824829, -0.40824829,  2.44948974]])
        
実世界データの前処理例

# 店舗の来客数情報
data = pd.read_csv("air_visit_data.csv", parse_dates=["visit_date"])
data

air_store_id	visit_date	visitors
0	air_ba937bf13d40fb24	2016-01-13	25
1	air_ba937bf13d40fb24	2016-01-14	32
2	air_ba937bf13d40fb24	2016-01-15	29
3	air_ba937bf13d40fb24	2016-01-16	22
4	air_ba937bf13d40fb24	2016-01-18	6
5	air_ba937bf13d40fb24	2016-01-19	9
6	air_ba937bf13d40fb24	2016-01-20	31
7	air_ba937bf13d40fb24	2016-01-21	21
8	air_ba937bf13d40fb24	2016-01-22	18
9	air_ba937bf13d40fb24	2016-01-23	26
10	air_ba937bf13d40fb24	2016-01-25	21
11	air_ba937bf13d40fb24	2016-01-26	11
12	air_ba937bf13d40fb24	2016-01-27	24
13	air_ba937bf13d40fb24	2016-01-28	21
14	air_ba937bf13d40fb24	2016-01-29	26
15	air_ba937bf13d40fb24	2016-01-30	6
16	air_ba937bf13d40fb24	2016-02-03	18
17	air_ba937bf13d40fb24	2016-02-04	12
18	air_ba937bf13d40fb24	2016-02-05	45
19	air_ba937bf13d40fb24	2016-02-06	15
20	air_ba937bf13d40fb24	2016-02-08	19
21	air_ba937bf13d40fb24	2016-02-09	15
22	air_ba937bf13d40fb24	2016-02-10	32
23	air_ba937bf13d40fb24	2016-02-11	3
24	air_ba937bf13d40fb24	2016-02-12	26
25	air_ba937bf13d40fb24	2016-02-13	8
26	air_ba937bf13d40fb24	2016-02-15	14
27	air_ba937bf13d40fb24	2016-02-16	15
28	air_ba937bf13d40fb24	2016-02-17	17
29	air_ba937bf13d40fb24	2016-02-18	22
...	...	...	...
252078	air_24e8414b9b07decb	2017-03-17	4
252079	air_24e8414b9b07decb	2017-03-18	7
252080	air_24e8414b9b07decb	2017-03-21	2
252081	air_24e8414b9b07decb	2017-03-22	8
252082	air_24e8414b9b07decb	2017-03-23	8
252083	air_24e8414b9b07decb	2017-03-24	7
252084	air_24e8414b9b07decb	2017-03-25	2
252085	air_24e8414b9b07decb	2017-03-27	4
252086	air_24e8414b9b07decb	2017-03-28	7
252087	air_24e8414b9b07decb	2017-03-29	11
252088	air_24e8414b9b07decb	2017-03-30	7
252089	air_24e8414b9b07decb	2017-03-31	8
252090	air_24e8414b9b07decb	2017-04-01	6
252091	air_24e8414b9b07decb	2017-04-03	4
252092	air_24e8414b9b07decb	2017-04-04	2
252093	air_24e8414b9b07decb	2017-04-05	2
252094	air_24e8414b9b07decb	2017-04-06	8
252095	air_24e8414b9b07decb	2017-04-07	8
252096	air_24e8414b9b07decb	2017-04-08	8
252097	air_24e8414b9b07decb	2017-04-10	4
252098	air_24e8414b9b07decb	2017-04-11	2
252099	air_24e8414b9b07decb	2017-04-12	8
252100	air_24e8414b9b07decb	2017-04-13	6
252101	air_24e8414b9b07decb	2017-04-14	2
252102	air_24e8414b9b07decb	2017-04-15	7
252103	air_24e8414b9b07decb	2017-04-18	6
252104	air_24e8414b9b07decb	2017-04-19	6
252105	air_24e8414b9b07decb	2017-04-20	7
252106	air_24e8414b9b07decb	2017-04-21	8
252107	air_24e8414b9b07decb	2017-04-22	5

252108 rows × 3 columns

# 後のプロットで横軸を時間表示するためにインデックス化
data.set_index('visit_date', inplace=True)
data

	air_store_id	visitors
visit_date		
2016-01-13	air_ba937bf13d40fb24	25
2016-01-14	air_ba937bf13d40fb24	32
2016-01-15	air_ba937bf13d40fb24	29
2016-01-16	air_ba937bf13d40fb24	22
2016-01-18	air_ba937bf13d40fb24	6
2016-01-19	air_ba937bf13d40fb24	9
2016-01-20	air_ba937bf13d40fb24	31
2016-01-21	air_ba937bf13d40fb24	21
2016-01-22	air_ba937bf13d40fb24	18
2016-01-23	air_ba937bf13d40fb24	26
2016-01-25	air_ba937bf13d40fb24	21
2016-01-26	air_ba937bf13d40fb24	11
2016-01-27	air_ba937bf13d40fb24	24
2016-01-28	air_ba937bf13d40fb24	21
2016-01-29	air_ba937bf13d40fb24	26
2016-01-30	air_ba937bf13d40fb24	6
2016-02-03	air_ba937bf13d40fb24	18
2016-02-04	air_ba937bf13d40fb24	12
2016-02-05	air_ba937bf13d40fb24	45
2016-02-06	air_ba937bf13d40fb24	15
2016-02-08	air_ba937bf13d40fb24	19
2016-02-09	air_ba937bf13d40fb24	15
2016-02-10	air_ba937bf13d40fb24	32
2016-02-11	air_ba937bf13d40fb24	3
2016-02-12	air_ba937bf13d40fb24	26
2016-02-13	air_ba937bf13d40fb24	8
2016-02-15	air_ba937bf13d40fb24	14
2016-02-16	air_ba937bf13d40fb24	15
2016-02-17	air_ba937bf13d40fb24	17
2016-02-18	air_ba937bf13d40fb24	22
...	...	...
2017-03-17	air_24e8414b9b07decb	4
2017-03-18	air_24e8414b9b07decb	7
2017-03-21	air_24e8414b9b07decb	2
2017-03-22	air_24e8414b9b07decb	8
2017-03-23	air_24e8414b9b07decb	8
2017-03-24	air_24e8414b9b07decb	7
2017-03-25	air_24e8414b9b07decb	2
2017-03-27	air_24e8414b9b07decb	4
2017-03-28	air_24e8414b9b07decb	7
2017-03-29	air_24e8414b9b07decb	11
2017-03-30	air_24e8414b9b07decb	7
2017-03-31	air_24e8414b9b07decb	8
2017-04-01	air_24e8414b9b07decb	6
2017-04-03	air_24e8414b9b07decb	4
2017-04-04	air_24e8414b9b07decb	2
2017-04-05	air_24e8414b9b07decb	2
2017-04-06	air_24e8414b9b07decb	8
2017-04-07	air_24e8414b9b07decb	8
2017-04-08	air_24e8414b9b07decb	8
2017-04-10	air_24e8414b9b07decb	4
2017-04-11	air_24e8414b9b07decb	2
2017-04-12	air_24e8414b9b07decb	8
2017-04-13	air_24e8414b9b07decb	6
2017-04-14	air_24e8414b9b07decb	2
2017-04-15	air_24e8414b9b07decb	7
2017-04-18	air_24e8414b9b07decb	6
2017-04-19	air_24e8414b9b07decb	6
2017-04-20	air_24e8414b9b07decb	7
2017-04-21	air_24e8414b9b07decb	8
2017-04-22	air_24e8414b9b07decb	5

252108 rows × 2 columns

# レストラン数チェック
len(np.unique(data["air_store_id"]))

829

# レストラン10店舗の来客数推移プロット
for i, store in enumerate(np.unique(data["air_store_id"])):
    if i == 10:
        break
    
    data[data["air_store_id"] == store].plot()
    plt.title(store)
    plt.show()
    
# air_0328696196e46f18は欠損がある店舗
data_store = data[data["air_store_id"] == "air_0328696196e46f18"].sort_index()
data_store.plot()
plt.show()

# 日付後半
data_store = data_store[-30:]
data_store

air_store_id	visitors
visit_date		
2017-03-01	air_0328696196e46f18	2
2017-03-02	air_0328696196e46f18	3
2017-03-05	air_0328696196e46f18	17
2017-03-09	air_0328696196e46f18	17
2017-03-13	air_0328696196e46f18	2
2017-03-14	air_0328696196e46f18	2
2017-03-16	air_0328696196e46f18	6
2017-03-17	air_0328696196e46f18	5
2017-03-19	air_0328696196e46f18	29
2017-03-20	air_0328696196e46f18	8
2017-03-21	air_0328696196e46f18	13
2017-03-22	air_0328696196e46f18	15
2017-03-23	air_0328696196e46f18	24
2017-03-27	air_0328696196e46f18	2
2017-03-29	air_0328696196e46f18	4
2017-03-30	air_0328696196e46f18	7
2017-03-31	air_0328696196e46f18	5
2017-04-01	air_0328696196e46f18	11
2017-04-03	air_0328696196e46f18	6
2017-04-04	air_0328696196e46f18	3
2017-04-05	air_0328696196e46f18	5
2017-04-11	air_0328696196e46f18	6
2017-04-12	air_0328696196e46f18	8
2017-04-14	air_0328696196e46f18	9
2017-04-15	air_0328696196e46f18	4
2017-04-16	air_0328696196e46f18	3
2017-04-17	air_0328696196e46f18	3
2017-04-19	air_0328696196e46f18	24
2017-04-21	air_0328696196e46f18	19
2017-04-22	air_0328696196e46f18	8

# 抜けている日付を挿入して、visitorsの値にはNaNを入れる
data_store = data_store.resample("D").mean()
data_store

	visitors
visit_date	
2017-03-01	2.0
2017-03-02	3.0
2017-03-03	NaN
2017-03-04	NaN
2017-03-05	17.0
2017-03-06	NaN
2017-03-07	NaN
2017-03-08	NaN
2017-03-09	17.0
2017-03-10	NaN
2017-03-11	NaN
2017-03-12	NaN
2017-03-13	2.0
2017-03-14	2.0
2017-03-15	NaN
2017-03-16	6.0
2017-03-17	5.0
2017-03-18	NaN
2017-03-19	29.0
2017-03-20	8.0
2017-03-21	13.0
2017-03-22	15.0
2017-03-23	24.0
2017-03-24	NaN
2017-03-25	NaN
2017-03-26	NaN
2017-03-27	2.0
2017-03-28	NaN
2017-03-29	4.0
2017-03-30	7.0
2017-03-31	5.0
2017-04-01	11.0
2017-04-02	NaN
2017-04-03	6.0
2017-04-04	3.0
2017-04-05	5.0
2017-04-06	NaN
2017-04-07	NaN
2017-04-08	NaN
2017-04-09	NaN
2017-04-10	NaN
2017-04-11	6.0
2017-04-12	8.0
2017-04-13	NaN
2017-04-14	9.0
2017-04-15	4.0
2017-04-16	3.0
2017-04-17	3.0
2017-04-18	NaN
2017-04-19	24.0
2017-04-20	NaN
2017-04-21	19.0
2017-04-22	8.0

# 日付が揃ったので線形補間が可能となる
data_store = data_store.interpolate()
data_store

	visitors
visit_date	
2017-03-01	2.000000
2017-03-02	3.000000
2017-03-03	7.666667
2017-03-04	12.333333
2017-03-05	17.000000
2017-03-06	17.000000
2017-03-07	17.000000
2017-03-08	17.000000
2017-03-09	17.000000
2017-03-10	13.250000
2017-03-11	9.500000
2017-03-12	5.750000
2017-03-13	2.000000
2017-03-14	2.000000
2017-03-15	4.000000
2017-03-16	6.000000
2017-03-17	5.000000
2017-03-18	17.000000
2017-03-19	29.000000
2017-03-20	8.000000
2017-03-21	13.000000
2017-03-22	15.000000
2017-03-23	24.000000
2017-03-24	18.500000
2017-03-25	13.000000
2017-03-26	7.500000
2017-03-27	2.000000
2017-03-28	3.000000
2017-03-29	4.000000
2017-03-30	7.000000
2017-03-31	5.000000
2017-04-01	11.000000
2017-04-02	8.500000
2017-04-03	6.000000
2017-04-04	3.000000
2017-04-05	5.000000
2017-04-06	5.166667
2017-04-07	5.333333
2017-04-08	5.500000
2017-04-09	5.666667
2017-04-10	5.833333
2017-04-11	6.000000
2017-04-12	8.000000
2017-04-13	8.500000
2017-04-14	9.000000
2017-04-15	4.000000
2017-04-16	3.000000
2017-04-17	3.000000
2017-04-18	13.500000
2017-04-19	24.000000
2017-04-20	21.500000
2017-04-21	19.000000
2017-04-22	8.000000

# 明日の来客数（目的変数）追加
data_store["y"] = data_store["visitors"].shift(-1)
data_store

	visitors	y
visit_date		
2017-03-01	2.000000	3.000000
2017-03-02	3.000000	7.666667
2017-03-03	7.666667	12.333333
2017-03-04	12.333333	17.000000
2017-03-05	17.000000	17.000000
2017-03-06	17.000000	17.000000
2017-03-07	17.000000	17.000000
2017-03-08	17.000000	17.000000
2017-03-09	17.000000	13.250000
2017-03-10	13.250000	9.500000
2017-03-11	9.500000	5.750000
2017-03-12	5.750000	2.000000
2017-03-13	2.000000	2.000000
2017-03-14	2.000000	4.000000
2017-03-15	4.000000	6.000000
2017-03-16	6.000000	5.000000
2017-03-17	5.000000	17.000000
2017-03-18	17.000000	29.000000
2017-03-19	29.000000	8.000000
2017-03-20	8.000000	13.000000
2017-03-21	13.000000	15.000000
2017-03-22	15.000000	24.000000
2017-03-23	24.000000	18.500000
2017-03-24	18.500000	13.000000
2017-03-25	13.000000	7.500000
2017-03-26	7.500000	2.000000
2017-03-27	2.000000	3.000000
2017-03-28	3.000000	4.000000
2017-03-29	4.000000	7.000000
2017-03-30	7.000000	5.000000
2017-03-31	5.000000	11.000000
2017-04-01	11.000000	8.500000
2017-04-02	8.500000	6.000000
2017-04-03	6.000000	3.000000
2017-04-04	3.000000	5.000000
2017-04-05	5.000000	5.166667
2017-04-06	5.166667	5.333333
2017-04-07	5.333333	5.500000
2017-04-08	5.500000	5.666667
2017-04-09	5.666667	5.833333
2017-04-10	5.833333	6.000000
2017-04-11	6.000000	8.000000
2017-04-12	8.000000	8.500000
2017-04-13	8.500000	9.000000
2017-04-14	9.000000	4.000000
2017-04-15	4.000000	3.000000
2017-04-16	3.000000	3.000000
2017-04-17	3.000000	13.500000
2017-04-18	13.500000	24.000000
2017-04-19	24.000000	21.500000
2017-04-20	21.500000	19.000000
2017-04-21	19.000000	8.000000
2017-04-22	8.000000	NaN

# 過去1週間の来客数（説明変数）追加
for i in range(1, 7):
    data_store["prev" + str(i)] = data_store["visitors"].shift(i)
    
data_store

	visitors	prev1	prev2	prev3	prev4	prev5	prev6
visit_date							
2017-03-01	2.000000	NaN	NaN	NaN	NaN	NaN	NaN
2017-03-02	3.000000	2.000000	NaN	NaN	NaN	NaN	NaN
2017-03-03	7.666667	3.000000	2.000000	NaN	NaN	NaN	NaN
2017-03-04	12.333333	7.666667	3.000000	2.000000	NaN	NaN	NaN
2017-03-05	17.000000	12.333333	7.666667	3.000000	2.000000	NaN	NaN
2017-03-06	17.000000	17.000000	12.333333	7.666667	3.000000	2.000000	NaN
2017-03-07	17.000000	17.000000	17.000000	12.333333	7.666667	3.000000	2.000000
2017-03-08	17.000000	17.000000	17.000000	17.000000	12.333333	7.666667	3.000000
2017-03-09	17.000000	17.000000	17.000000	17.000000	17.000000	12.333333	7.666667
2017-03-10	13.250000	17.000000	17.000000	17.000000	17.000000	17.000000	12.333333
2017-03-11	9.500000	13.250000	17.000000	17.000000	17.000000	17.000000	17.000000
2017-03-12	5.750000	9.500000	13.250000	17.000000	17.000000	17.000000	17.000000
2017-03-13	2.000000	5.750000	9.500000	13.250000	17.000000	17.000000	17.000000
2017-03-14	2.000000	2.000000	5.750000	9.500000	13.250000	17.000000	17.000000
2017-03-15	4.000000	2.000000	2.000000	5.750000	9.500000	13.250000	17.000000
2017-03-16	6.000000	4.000000	2.000000	2.000000	5.750000	9.500000	13.250000
2017-03-17	5.000000	6.000000	4.000000	2.000000	2.000000	5.750000	9.500000
2017-03-18	17.000000	5.000000	6.000000	4.000000	2.000000	2.000000	5.750000
2017-03-19	29.000000	17.000000	5.000000	6.000000	4.000000	2.000000	2.000000
2017-03-20	8.000000	29.000000	17.000000	5.000000	6.000000	4.000000	2.000000
2017-03-21	13.000000	8.000000	29.000000	17.000000	5.000000	6.000000	4.000000
2017-03-22	15.000000	13.000000	8.000000	29.000000	17.000000	5.000000	6.000000
2017-03-23	24.000000	15.000000	13.000000	8.000000	29.000000	17.000000	5.000000
2017-03-24	18.500000	24.000000	15.000000	13.000000	8.000000	29.000000	17.000000
2017-03-25	13.000000	18.500000	24.000000	15.000000	13.000000	8.000000	29.000000
2017-03-26	7.500000	13.000000	18.500000	24.000000	15.000000	13.000000	8.000000
2017-03-27	2.000000	7.500000	13.000000	18.500000	24.000000	15.000000	13.000000
2017-03-28	3.000000	2.000000	7.500000	13.000000	18.500000	24.000000	15.000000
2017-03-29	4.000000	3.000000	2.000000	7.500000	13.000000	18.500000	24.000000
2017-03-30	7.000000	4.000000	3.000000	2.000000	7.500000	13.000000	18.500000
2017-03-31	5.000000	7.000000	4.000000	3.000000	2.000000	7.500000	13.000000
2017-04-01	11.000000	5.000000	7.000000	4.000000	3.000000	2.000000	7.500000
2017-04-02	8.500000	11.000000	5.000000	7.000000	4.000000	3.000000	2.000000
2017-04-03	6.000000	8.500000	11.000000	5.000000	7.000000	4.000000	3.000000
2017-04-04	3.000000	6.000000	8.500000	11.000000	5.000000	7.000000	4.000000
2017-04-05	5.000000	3.000000	6.000000	8.500000	11.000000	5.000000	7.000000
2017-04-06	5.166667	5.000000	3.000000	6.000000	8.500000	11.000000	5.000000
2017-04-07	5.333333	5.166667	5.000000	3.000000	6.000000	8.500000	11.000000
2017-04-08	5.500000	5.333333	5.166667	5.000000	3.000000	6.000000	8.500000
2017-04-09	5.666667	5.500000	5.333333	5.166667	5.000000	3.000000	6.000000
2017-04-10	5.833333	5.666667	5.500000	5.333333	5.166667	5.000000	3.000000
2017-04-11	6.000000	5.833333	5.666667	5.500000	5.333333	5.166667	5.000000
2017-04-12	8.000000	6.000000	5.833333	5.666667	5.500000	5.333333	5.166667
2017-04-13	8.500000	8.000000	6.000000	5.833333	5.666667	5.500000	5.333333
2017-04-14	9.000000	8.500000	8.000000	6.000000	5.833333	5.666667	5.500000
2017-04-15	4.000000	9.000000	8.500000	8.000000	6.000000	5.833333	5.666667
2017-04-16	3.000000	4.000000	9.000000	8.500000	8.000000	6.000000	5.833333
2017-04-17	3.000000	3.000000	4.000000	9.000000	8.500000	8.000000	6.000000
2017-04-18	13.500000	3.000000	3.000000	4.000000	9.000000	8.500000	8.000000
2017-04-19	24.000000	13.500000	3.000000	3.000000	4.000000	9.000000	8.500000
2017-04-20	21.500000	24.000000	13.500000	3.000000	3.000000	4.000000	9.000000
2017-04-21	19.000000	21.500000	24.000000	13.500000	3.000000	3.000000	4.000000
2017-04-22	8.000000	19.000000	21.500000	24.000000	13.500000	3.000000	3.000000

