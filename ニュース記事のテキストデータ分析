import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 正規表現用
import re

# scikit-learn
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn import datasets, manifold, mixture
from sklearn.model_selection import cross_validate

# gensimの言語系ライブラリ
from gensim.models import Word2Vec
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

# Random Forest
from sklearn.ensemble import RandomForestClassifier

# 今回扱うニュース記事のジャンル
categories = [
    'alt.atheism',  # 無神論
    'comp.graphics',  # 画像
    'rec.sport.baseball',  # 野球
    'sci.space',  # 宇宙
    'talk.politics.guns'  # 銃の所持・使用に関する政治
]

# ライブラリで用意されているデータをダウンロード
train = datasets.fetch_20newsgroups(subset='train', categories=categories)

# 分析用にデータ形式を変換
train.data = np.array(train.data, dtype=np.object)

# 各ジャンルのデータ数確認
for i, c in enumerate(categories):
    indices = np.where(train.target == i)
    print(c, len(train.data[indices]))
    
alt.atheism 480
comp.graphics 584
rec.sport.baseball 597
sci.space 593
talk.politics.guns 546

# 文章の前処理
def analyzer(text):
    
    # 無視する単語
    stop_words = ['i', 'a', 'an', 'the', 'to', 'and', 'or', 'if', 'is', 'are', 'am', 'it', 'this', 'that', 'of', 'from', 'in', 'on']
    
    text = text.lower() # 小文字化
    text = text.replace('\n', '') # 改行削除
    text = text.replace('\t', '') # タブ削除
    text = re.sub(re.compile(r'[!-/:-@[-`{-~]'), ' ', text) # 記号をスペースに置き換え
    text = text.split(' ') # スペースで区切る
    
    words = []
    for word in text:
        if (re.compile(r'^.*[0-9]+.*$').fullmatch(word) is not None): # 数字が含まれるものは除外
            continue
        if word in stop_words: # ストップワードに含まれるものは除外
            continue
        if len(word) < 2: #  1文字、0文字（空文字）は除外
            continue
        words.append(word)
        
    return words
    

[ ]
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 正規表現用
import re

# scikit-learn
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn import datasets, manifold, mixture
from sklearn.model_selection import cross_validate

# gensimの言語系ライブラリ
from gensim.models import Word2Vec
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

# Random Forest
from sklearn.ensemble import RandomForestClassifier
[ ]
# 今回扱うニュース記事のジャンル
categories = [
    'alt.atheism',  # 無神論
    'comp.graphics',  # 画像
    'rec.sport.baseball',  # 野球
    'sci.space',  # 宇宙
    'talk.politics.guns'  # 銃の所持・使用に関する政治
]

# ライブラリで用意されているデータをダウンロード
train = datasets.fetch_20newsgroups(subset='train', categories=categories)

# 分析用にデータ形式を変換
train.data = np.array(train.data, dtype=np.object)
[ ]
# 各ジャンルのデータ数確認
for i, c in enumerate(categories):
    indices = np.where(train.target == i)
    print(c, len(train.data[indices]))


[ ]
# 文章の前処理
def analyzer(text):
    
    # 無視する単語
    stop_words = ['i', 'a', 'an', 'the', 'to', 'and', 'or', 'if', 'is', 'are', 'am', 'it', 'this', 'that', 'of', 'from', 'in', 'on']
    
    text = text.lower() # 小文字化
    text = text.replace('\n', '') # 改行削除
    text = text.replace('\t', '') # タブ削除
    text = re.sub(re.compile(r'[!-/:-@[-`{-~]'), ' ', text) # 記号をスペースに置き換え
…        words.append(word)
        
    return words
    
TF-IDF    

# TfidfVectorizerにて前処理のanalyzer関数を指定するため、そのまま渡せばOK
corpus = train.data

# モデルを定義
tfidf_vectorizer = TfidfVectorizer(analyzer=analyzer, min_df=10) # 単語が出現する文章数がmin_df未満のとき、当該単語を無視

# 定義したモデルでTF-IDFによる文章ベクトル化処理実行
tfidfs = tfidf_vectorizer.fit_transform(corpus)

tfidfs.shape

(2800, 5445)

# 語彙の確認（右の数字は登録順）
tfidf_vectorizer.vocabulary_

{'ab': 3,
 'nova': 3283,
 'cc': 769,
 'purdue': 3844,
 'edu': 1503,
 'allen': 150,
 'subject': 4668,
 're': 3922,
 'tiff': 4903,
 'philosophical': 3571,
 'significance': 4406,
 'universitylines': 5077,
 'article': 292,
 'de': 1224,
 'writes': 5399,
 'according': 35,
 'version': 5171,
 'number': 3294,
 'bytes': 678,
 'has': 2158,
 'been': 474,
 'chosen': 836,
 'for': 1876,
 'its': 2505,
 'deep': 1250,
 'last': 2671,
 'week': 5285,
 'read': 3927,
 'guide': 2112,
 'galaxy': 1966,
 'actually': 62,
 'how': 2267,
 'they': 4863,
 'picked': 3585,
 'sure': 4717,
 'not': 3273,
 'every': 1640,
 'time': 4909,
 'part': 3497,
 'spec': 4518,
 'me': 2963,
 'none': 3261,
 'too': 4935,
 'happy': 2149,
 'about': 10,
 'anyway': 224,
 'because': 469,
 'think': 4868,
 'their': 4840,
 'arbitrary': 259,
 'neither': 3211,
 'find': 1815,
 'begin': 479,
 'file': 1804,
 'with': 5348,
 'meaningless': 2967,
 'themselves': 4845,
 'just': 2578,
 'use': 5109,
 'letters': 2728,
 'no': 3253,
 'don': 1417,
 'should': 4384,
 'have': 2162,
 'bothered': 590,
 'support': 4708,
 'both': 588,
 'either': 1537,
 've': 5158,
 'found': 1904,
 'many': 2905,
 'readers': 3929,
 'sandvik': 4213,
 'newton': 3237,
 'apple': 239,
 'com': 931,
 'kent': 2595,
 'objective': 3306,
 'morality': 3106,
 'was': 5259,
 'political': 3651,
 'atheists': 340,
 'organization': 3409,
 'cookamunga': 1088,
 'tourist': 4950,
 'bureaulines': 659,
 'gap': 1972,
 'caltech': 696,
 'keith': 2589,
 'cco': 771,
 'schneider': 4246,
 'wrote': 5404,
 'which': 5309,
 'type': 5019,
 'you': 5427,
 'talking': 4757,
 'natural': 3183,
 'sense': 4312,
 'at': 335,
 'all': 144,
 'immoral': 2344,
 'harm': 2154,
 'another': 212,
 'species': 4520,
 'as': 296,
 'long': 2810,
 'doesn': 1409,
 'affect': 95,
 'your': 5432,
 'own': 3449,
 'guess': 2109,
 'so': 4464,
 'say': 4231,
 'but': 671,
 'tells': 4802,
 'case': 748,
 'know': 2634,
 'rules': 4169,
 'game': 1968,
 'systems': 4746,
 'cheers': 820,
 'alink': 142,
 'ksand': 2644,
 'private': 3751,
 'activities': 58,
 'net': 3216,
 'tamu': 4760,
 'edusubject': 1518,
 'help': 2193,
 'need': 3203,
 'graphics': 2077,
 'code': 909,
 'package': 3462,
 'dos': 1424,
 'texas': 4826,
 'posting': 3685,
 'host': 2260,
 'eduin': 1510,
 'unit': 5068,
 'whatever': 5302,
 'take': 4750,
 'data': 1207,
 'turn': 5004,
 'into': 2463,
 'wireframe': 5341,
 'surface': 4719,
 'hidden': 2207,
 'lines': 2768,
 'removed': 4017,
 'using': 5117,
 'machine': 2855,
 'can': 705,
 'be': 456,
 'fortran': 1897,
 'basic': 436,
 'forms': 1893,
 'general': 1986,
 'interest': 2448,
 'question': 3864,
 'thank': 4832,
 'afraid': 98,
 'reply': 4030,
 'didn': 1342,
 'get': 2007,
 'thru': 4893,
 'do': 1399,
 'appreciate': 247,
 'trying': 4999,
 'however': 2269,
 'please': 3634,
 'try': 4998,
 'again': 102,
 'po': 3642,
 'cwru': 1186,
 'america': 176,
 'team': 4778,
 'why': 5317,
 'western': 5300,
 'reserve': 4053,
 'university': 5074,
 'cleveland': 883,
 'oh': 3349,
 'usa': 5103,
 'nntp': 3252,
 'ins': 2420,
 'previous': 3732,
 'utoronto': 5129,
 'ca': 680,
 'david': 1213,
 'says': 4233,
 'uxa': 5137,
 'cso': 1163,
 'uiuc': 5041,
 'talent': 4754,
 'comes': 937,
 'out': 3432,
 'nowhere': 3286,
 'contend': 1053,
 'mets': 3011,
 'orioles': 3419,
 'prime': 3738,
 'examples': 1654,
 'cubs': 1169,
 'sorry': 4496,
 'virtually': 5205,
 'impossible': 2359,
 'win': 5329,
 'division': 1395,
 'over': 3443,
 'games': 1969,
 'would': 5386,
 'amend': 174,
 'definition': 1265,
 'expected': 1679,
 'lose': 2820,
 'wins': 5338,
 'thanks': 4833,
 'dave': 1212,
 'ok': 3353,
 'good': 2048,
 'what': 5301,
 'reds': 3960,
 'anyone': 222,
 'them': 4844,
 'sweep': 4737,
 'people': 3537,
 'even': 1634,
 'let': 2724,
 'alone': 158,
 'series': 4325,
 'we': 5276,
 'proved': 3815,
 'wrong': 5403,
 'though': 4877,
 'year': 5416,
 'ignore': 2324,
 'record': 3955,
 'now': 3285,
 'had': 2121,
 'start': 4578,
 'nothing': 3277,
 'colorado': 923,
 'shall': 4354,
 'rise': 4117,
 'hunt': 2287,
 'fora': 1877,
 'october': 3328,
 'continues': 1059,
 'due': 1460,
 'respect': 4061,
 'spring': 4551,
 'race': 3878,
 'pull': 3834,
 'wars': 5258,
 'spirit': 4540,
 'love': 2832,
 'feeling': 1774,
 'hall': 2126,
 'jmd': 2539,
 'cube': 1167,
 'handheld': 2135,
 'jim': 2537,
 'arras': 283,
 'atf': 336,
 'burns': 663,
 'dividian': 1393,
 'ranch': 3894,
 'survivors': 4732,
 'hand': 2130,
 'held': 2190,
 'products': 3772,
 'inc': 2367,
 'worldnntp': 5376,
 'dale': 1193,
 'comin': 940,
 'rice': 4096,
 'green': 2085,
 'bill': 524,
 'shed': 4362,
 'some': 4484,
 'light': 2755,
 'fire': 1824,
 'widely': 5319,
 'reported': 4033,
 'ap': 227,
 'etc': 1626,
 'there': 4854,
 'were': 5296,
 'several': 4342,
 'witnesses': 5354,
 'bd': 454,
 'folks': 1864,
 'starting': 4582,
 'fires': 1829,
 'also': 163,
 'broke': 625,
 'places': 3610,
 'once': 3363,
 'bradley': 601,
 'knocking': 2633,
 'lamp': 2656,
 'cause': 762,
 'consider': 1032,
 'bds': 455,
 'more': 3108,
 'than': 4831,
 'one': 3364,
 'tanks': 4762,
 'made': 2861,
 'hole': 2238,
 'building': 645,
 'did': 1341,
 'else': 1548,
 'notice': 3278,
 'video': 5186,
 'appeared': 237,
 'smoke': 4458,
 'coming': 941,
 'tank': 4761,
 'nearby': 3197,
 'fact': 1720,
 'appears': 238,
 'started': 4579,
 'does': 1408,
 'rule': 4167,
 'anything': 223,
 'watched': 5267,
 'live': 2785,
 'times': 4911,
 'press': 3724,
 'point': 3643,
 'only': 3368,
 'visible': 5207,
 'where': 5306,
 'side': 4396,
 'across': 51,
 'whole': 5314,
 'dry': 1454,
 'structure': 4650,
 'minutes': 3051,
 'by': 676,
 'other': 3423,
 'nor': 3264,
 'needed': 3204,
 'flame': 1842,
 'observed': 3314,
 'these': 4859,
 'way': 5273,
 'heard': 2181,
 'fbi': 1762,
 'cnn': 901,
 'driving': 1448,
 'fan': 1742,
 'janet': 2515,
 'reno': 4021,
 'like': 2758,
 'she': 4360,
 'balls': 417,
 'go': 2035,
 'ahead': 116,
 'full': 1944,
 'responsibility': 4068,
 'seems': 4293,
 'boy': 599,
 'problems': 3761,
 'figuring': 1803,
 'he': 2170,
 'stood': 4618,
 'issue': 2493,
 'bad': 409,
 'will': 5323,
 'happen': 2144,
 'her': 2200,
 'him': 2217,
 'media': 2979,
 'done': 1420,
 'job': 2540,
 'well': 5293,
 'yep': 5420,
 'without': 5351,
 'sad': 4196,
 'always': 171,
 'rethinking': 4080,
 'never': 3225,
 'day': 1218,
 'when': 5304,
 'haven': 2164,
 'rethought': 4081,
 'myself': 3164,
 'clinton': 885,
 'april': 256,
 'american': 177,
 'englishman': 1578,
 'while': 5310,
 'foreign': 1882,
 'troop': 4988,
 'landed': 2659,
 'my': 3162,
 'country': 1117,
 'lay': 2689,
 'down': 1430,
 'arms': 280,
 'william': 5325,
 'pitt': 3604,
 'earl': 1476,
 'chatham': 813,
 'nov': 3282,
 'watson': 5271,
 'ibm': 2299,
 'represents': 4042,
 'poster': 3683,
 'views': 5193,
 'necessarily': 3200,
 'those': 4876,
 'comorganization': 958,
 'few': 1785,
 'left': 2711,
 'book': 578,
 'future': 1960,
 'include': 2374,
 'probably': 3757,
 'forum': 1900,
 'including': 2377,
 'current': 1175,
 'techniques': 4785,
 'development': 1332,
 'computer': 991,
 'art': 290,
 'career': 733,
 'programmer': 3777,
 'stewart': 4611,
 'jeffrey': 2526,
 'send': 4308,
 'free': 1924,
 'purposes': 3848,
 'until': 5086,
 'run': 4173,
 'name': 3168,
 'address': 71,
 'mathew': 2942,
 'mantis': 2901,
 'co': 902,
 'uk': 5042,
 'alt': 164,
 'atheism': 337,
 'faq': 1747,
 'introduction': 2466,
 'before': 476,
 'thu': 4894,
 'may': 2952,
 'gmtdistribution': 2030,
 'worldorganization': 5377,
 'consultants': 1046,
 'cambridge': 697,
 'archive': 264,
 'modified': 3081,
 'signed': 4405,
 'message': 3003,
 'attempts': 359,
 'provide': 3817,
 'ihave': 2327,
 'tried': 4982,
 'possible': 3679,
 'regarding': 3975,
 'issues': 2495,
 'remember': 4011,
 'document': 1401,
 'viewpoint': 5192,
 'encourage': 1560,
 'draw': 1436,
 'conclusions': 1008,
 'books': 579,
 'listed': 2776,
 'presented': 3722,
 'between': 512,
 'atheist': 339,
 'theist': 4842,
 'asked': 302,
 'questions': 3865,
 'since': 4427,
 'newsgroup': 3230,
 'created': 1140,
 'answered': 214,
 'note': 3274,
 'towards': 4953,
 'christian': 839,
 'files': 1806,
 'talk': 4755,
 'religion': 4002,
 'primarily': 3736,
 'religions': 4003,
 'such': 4677,
 'islam': 2485,
 'involve': 2472,
 'sort': 4497,
 'much': 3146,
 'discussion': 1377,
 'apply': 246,
 'absence': 13,
 'belief': 487,
 'existence': 1670,
 'god': 2038,
 'further': 1957,
 'believe': 489,
 'exist': 1667,
 'former': 1891,
 'referred': 3968,
 'weak': 5277,
 'position': 3672,
 'latter': 2675,
 'important': 2355,
 'difference': 1349,
 'two': 5016,
 'positions': 3673,
 'simple': 4421,
 'positive': 3674,
 'assuming': 323,
 'strong': 4647,
 'non': 3260,
 'gods': 2040,
 'others': 3424,
 'limit': 2763,
 'specific': 4521,
 'rather': 3911,
 'isn': 2489,
 'same': 4208,
 'thing': 4866,
 'believing': 494,
 'definitely': 1264,
 'means': 2968,
 'true': 4992,
 'something': 4488,
 'equivalent': 1603,
 'false': 1736,
 'simply': 4423,
 'idea': 2306,
 'whether': 5308,
 'brings': 619,
 'us': 5102,
 'then': 4846,
 'term': 4814,
 'professor': 3774,
 'meeting': 2984,
 'society': 4468,
 'defined': 1261,
 'someone': 4487,
 'believed': 490,
 'ultimate': 5047,
 'origin': 3415,
 'unknown': 5080,
 'thus': 4897,
 'who': 5312,
 'believes': 493,
 'cannot': 713,
 'exists': 1672,
 'words': 5366,
 'things': 4867,
 'language': 2662,
 'thatyou': 4837,
 'work': 5367,
 'view': 5187,
 'calls': 694,
 'example': 1653,
 'mean': 2964,
 'word': 5364,
 'meaning': 2965,
 'itis': 2504,
 'very': 5175,
 'difficult': 1353,
 'certainly': 787,
 'science': 4250,
 'best': 509,
 'universe': 5072,
 'justification': 2580,
 'basis': 438,
 'person': 3555,
 'ask': 301,
 'feel': 1773,
 'major': 2879,
 'essentially': 1618,
 'self': 4301,
 'contradictory': 1065,
 'logically': 2807,
 'could': 1111,
 'through': 4888,
 'evidence': 1646,
 'prove': 3814,
 'counter': 1115,
 'statement': 4587,
 'larger': 2667,
 'numbers': 3295,
 'course': 1121,
 'deals': 1229,
 'matter': 2946,
 'debate': 1234,
 'moment': 3089,
 'still': 4613,
 'reasons': 3944,
 'assume': 320,
 'assumption': 324,
 'finding': 1816,
 'single': 4430,
 'showing': 4389,
 'require': 4047,
 'search': 4271,
 'show': 4387,
 'problem': 3760,
 'therefore': 4855,
 'generally': 1987,
 'accepted': 29,
 'must': 3160,
 'theists': 4843,
 'follow': 1865,
 'most': 3114,
 'ofthe': 3348,
 'unicorns': 5063,
 'anywhere': 226,
 'make': 2882,
 'everywhere': 1645,
 'might': 3024,
 'assumes': 322,
 'test': 4820,
 'usually': 5122,
 'claim': 866,
 'instead': 2426,
 'restrict': 4073,
 'claims': 869,
 'described': 1300,
 'followers': 1867,
 'various': 5154,
 'particular': 3501,
 'any': 218,
 'present': 3721,
 'practice': 3699,
 'sufficiently': 4687,
 'arguments': 274,
 'based': 433,
 'kind': 2615,
 'really': 3938,
 'applicable': 241,
 'our': 3429,
 'effects': 1529,
 'his': 2220,
 'hence': 2198,
 'bible': 517,
 'easily': 1484,
 'surely': 4718,
 'today': 4922,
 'physical': 3580,
 'caused': 763,
 'presence': 3720,
 'otherwise': 3425,
 'distinguish': 1389,
 'religious': 4004,
 'common': 951,
 'cynical': 1189,
 'follows': 1869,
 'begins': 481,
 'making': 2884,
 'points': 3647,
 'defines': 1262,
 'used': 5110,
 'prepared': 3718,
 'accept': 26,
 'records': 3956,
 'along': 159,
 'agreed': 113,
 'eventually': 1638,
 'uses': 5116,
 'original': 3416,
 'definitions': 1266,
 'originally': 3417,
 'agree': 112,
 'apparently': 233,
 'inconsistent': 2378,
 'tend': 4808,
 'play': 3627,
 'answer': 213,
 'depends': 1291,
 'upon': 5095,
 'meant': 2969,
 'power': 3694,
 'especially': 1614,
 'faith': 1732,
 'worth': 5383,
 'pointing': 3646,
 'passing': 3512,
 'meaningful': 2966,
 'tends': 4809,
 'result': 4076,
 'aspects': 306,
 'behaviour': 483,
 'suddenly': 4681,
 'becoming': 472,
 'politics': 3654,
 'watching': 5268,
 'tv': 5009,
 'act': 53,
 'entirely': 1588,
 'clear': 877,
 'secondly': 4280,
 'necessary': 3201,
 'core': 1097,
 'beliefs': 488,
 'assumptions': 325,
 'tomake': 4928,
 'experience': 1682,
 'doubt': 1426,
 'laws': 2687,
 'physics': 3582,
 'observers': 3316,
 'ideas': 2308,
 'called': 692,
 'acts': 60,
 'almost': 156,
 'everything': 1644,
 'said': 4201,
 'often': 3347,
 'refer': 3965,
 'complete': 978,
 'certain': 786,
 'individual': 2395,
 'scientists': 4256,
 'claiming': 868,
 'supporting': 4711,
 'proof': 3793,
 'fit': 1835,
 'closer': 892,
 'match': 2935,
 'experimental': 1685,
 'lack': 2651,
 'anti': 217,
 'human': 2280,
 'everyone': 1643,
 'against': 103,
 'friend': 1935,
 'truth': 4996,
 'cut': 1181,
 'runs': 4177,
 'theism': 4841,
 'being': 485,
 'hostile': 2261,
 'quite': 3869,
 'cover': 1126,
 'broad': 623,
 'spectrum': 4524,
 'attitude': 363,
 'unless': 5081,
 'mention': 2994,
 'except': 1656,
 'perhaps': 3547,
 'close': 889,
 'friends': 1936,
 'ofcourse': 3332,
 'acceptable': 27,
 'countries': 1116,
 'convert': 1076,
 'outside': 3441,
 'eastern': 1486,
 'slightly': 4450,
 'soviet': 4508,
 'union': 5066,
 'dedicated': 1249,
 'church': 844,
 'state': 4585,
 'citizens': 859,
 'legally': 2714,
 'came': 698,
 'took': 4936,
 'control': 1070,
 'destroy': 1317,
 'gain': 1963,
 'population': 3667,
 'matters': 2947,
 'business': 668,
 'government': 2060,
 'individuals': 2396,
 'concerned': 1002,
 'remain': 4006,
 'separate': 4317,
 'allow': 152,
 'running': 4176,
 'principle': 3742,
 'separation': 4319,
 'concerning': 1003,
 'promote': 3791,
 'expense': 1680,
 'purely': 3846,
 'believers': 492,
 'responsible': 4069,
 'increase': 2380,
 'spending': 4534,
 'aid': 118,
 'founded': 1906,
 'campaign': 703,
 'grounds': 2096,
 'see': 4287,
 'schools': 4248,
 'care': 732,
 'pray': 3702,
 'voters': 5224,
 'christians': 841,
 'told': 4926,
 'join': 2548,
 'public': 3828,
 'event': 1636,
 'family': 1740,
 'wasting': 5265,
 'mentioned': 2995,
 'increased': 2381,
 'aren': 268,
 'object': 3305,
 'purpose': 3847,
 'contribute': 1067,
 'forthe': 1896,
 'sake': 4204,
 'practical': 3698,
 'seem': 4291,
 'obvious': 3320,
 'somewhat': 4491,
 'cheap': 814,
 'excuse': 1661,
 'strange': 4631,
 'holding': 2236,
 'party': 3505,
 'rarely': 3906,
 'parents': 3490,
 'attempt': 356,
 'call': 691,
 'chose': 835,
 'different': 1351,
 'theother': 4852,
 'choose': 833,
 'conclude': 1005,
 'although': 169,
 'group': 2097,
 'independently': 2388,
 'differ': 1348,
 'listen': 2777,
 'heavy': 2187,
 'metal': 3006,
 'prefer': 3712,
 'black': 541,
 'carry': 743,
 'around': 282,
 'arguing': 272,
 'whoever': 5313,
 'chances': 794,
 'met': 3005,
 'less': 2723,
 'moral': 3105,
 'define': 1260,
 'talks': 4758,
 'right': 4111,
 'within': 5350,
 'humans': 2282,
 'social': 4466,
 'animals': 204,
 'successful': 4675,
 'operate': 3376,
 'each': 1474,
 'enough': 1581,
 'reason': 3940,
 'happens': 2147,
 'naturally': 3184,
 'justify': 2582,
 'actions': 56,
 'equally': 1599,
 'here': 2201,
 'saying': 4232,
 'deserves': 1307,
 'acceptance': 28,
 'jesus': 2530,
 'christ': 838,
 'world': 5374,
 'save': 4226,
 'shown': 4390,
 'mercy': 2999,
 'display': 1382,
 'receive': 3947,
 'eternal': 1627,
 'life': 2752,
 'king': 2618,
 'honor': 2250,
 'glory': 2026,
 'forever': 1883,
 'ever': 1639,
 'above': 12,
 'quote': 3871,
 'court': 1122,
 'february': 1768,
 'serial': 4324,
 'milwaukee': 3035,
 'wisconsin': 5343,
 'mass': 2931,
 'trivial': 4987,
 'survey': 4728,
 'conducted': 1012,
 'behavior': 482,
 'after': 99,
 'born': 585,
 'driven': 1444,
 'conversion': 1075,
 'similarly': 4419,
 'illegal': 2333,
 'drugs': 1453,
 'percent': 3539,
 'admitted': 81,
 'sex': 4345,
 'september': 4322,
 'yes': 5421,
 'explained': 1691,
 'least': 2704,
 'itself': 2506,
 'imply': 2352,
 'ought': 3427,
 'seeing': 4288,
 'set': 4336,
 'study': 4661,
 'freedom': 1925,
 'foundation': 1905,
 'responded': 4065,
 'became': 468,
 'ignorance': 2322,
 'choice': 831,
 'vast': 5155,
 'majority': 2880,
 'spent': 4535,
 'sometimes': 4490,
 'great': 2081,
 'depth': 1294,
 'careful': 734,
 'considered': 1035,
 'decision': 1244,
 'reject': 3987,
 'inevitable': 2399,
 'consequence': 1029,
 'makes': 2883,
 'honestly': 2249,
 'want': 5250,
 'lives': 2788,
 'nobody': 3254,
 'desire': 1313,
 'big': 519,
 'brother': 628,
 'figure': 1800,
 'able': 8,
 'merely': 3001,
 'wants': 5252,
 'risks': 4119,
 'approach': 249,
 'decide': 1241,
 'previously': 3733,
 'seriously': 4327,
 'possibility': 3678,
 'reach': 3923,
 'open': 3374,
 'mind': 3038,
 'biased': 516,
 'offensive': 3335,
 'closed': 890,
 'comments': 946,
 'looking': 2814,
 'properly': 3796,
 'likely': 2761,
 'viewed': 5188,
 'wish': 5346,
 'give': 2016,
 'benefit': 503,
 'basically': 437,
 'telling': 4801,
 'completely': 980,
 'goals': 2037,
 'having': 2166,
 'influence': 2404,
 'hope': 2252,
 'leaving': 2707,
 'mark': 2916,
 'history': 2222,
 'look': 2812,
 'put': 3852,
 'looks': 2815,
 'asking': 303,
 'silly': 4416,
 'cup': 1173,
 'danger': 1200,
 'ways': 5275,
 'level': 2730,
 'food': 1870,
 'drink': 1442,
 'sound': 4500,
 'empty': 1558,
 'face': 1715,
 'reality': 3936,
 'end': 1561,
 'searching': 4272,
 'luck': 2841,
 'worry': 5380,
 'short': 4376,
 'years': 5417,
 ...}
 
 # ある語彙のベクトルを確認
for i in tfidfs[0].toarray()[0]:
    print(i)
    
0.0
0.0
0.0
0.19211909388233292
0.0
0.0
0.0
0.0
0.0
0.0
0.038182653166853105
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.08473860493181404
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.06304131685274514
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.09082343616160615
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.07963996244943279
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.11714108457063516
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.032551968806915826
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.053688568862997124
0.0
0.0
0.0
0.0
0.04788083688491525
0.0
0.0
0.0
0.0
0.10024766754774994
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.06517565253797623
0.0
0.11604425790609049
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.2417335828322403
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.07082188514074575
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.2417335828322403
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.1470581531508741
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.09889256992515977
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.04125469903457123
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.025952504397508878
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.06570978124602153
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.06502573581836389
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.07855202175055186
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.06011215979656417
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.025884405620054292
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0719662982959089
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.11308622312811954
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.1082636896180899
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.09053914588769897
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.04097549975824196
0.0
0.0
0.0
0.029827060608369784
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.04369691071378247
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.05993954175841593
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.041714474700213604
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.05829374141657639
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.10328386635439803
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.05414896535812707
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.041648114141255514
0.0
0.0
0.0
0.11714108457063516
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.09082343616160615
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.03993799073755553
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.08625719016744494
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.03157387598314041
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.11402246942036957
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.207811183953761
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.06698953820446521
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.21948102740023107
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.1082636896180899
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.20341155908787195
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.024309641334491535
0.0
0.0
0.0
0.0
0.12568076108043624
0.0
0.10384011522916707
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.052408811904553446
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.2151316441493041
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.2476496523881822
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.020493110715666882
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.06908498989967811
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.06167703997022967
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.04785030506859673
0.0
0.0
0.0
0.0
0.08372306413620345
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.07204621859532205
0.0
0.0
0.0
0.0
0.0880380202905651
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.5036075003945074
0.0
0.0
0.0
0.0
0.0
0.04862856430465959
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.05587649901434473
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0764097352707144
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.05410665168605795
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.05634569702246737
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.06853928746238694
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.08166175093515371
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.03241500004189754
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.029862872579898492
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0

# 高次元ベクトルを2次元に圧縮
tsne_tfidf = manifold.TSNE(n_components=2).fit_transform(tfidfs.toarray())
tsne_tfidf.shape

(2800, 2)

# 二次元グラフでプロット
df_tsne_tfidf = pd.DataFrame({
    'x': tsne_tfidf[:, 0],
    'y': tsne_tfidf[:, 1],
    'category': train.target,
})
 
df_tsne_tfidf.plot.scatter(x='x', y='y', c='category', colormap='viridis', figsize=(7, 5), s=20)
plt.show()


[ ]
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 正規表現用
import re

# scikit-learn
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn import datasets, manifold, mixture
from sklearn.model_selection import cross_validate

# gensimの言語系ライブラリ
from gensim.models import Word2Vec
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

# Random Forest
from sklearn.ensemble import RandomForestClassifier
[ ]
# 今回扱うニュース記事のジャンル
categories = [
    'alt.atheism',  # 無神論
    'comp.graphics',  # 画像
    'rec.sport.baseball',  # 野球
    'sci.space',  # 宇宙
    'talk.politics.guns'  # 銃の所持・使用に関する政治
]

# ライブラリで用意されているデータをダウンロード
train = datasets.fetch_20newsgroups(subset='train', categories=categories)

# 分析用にデータ形式を変換
train.data = np.array(train.data, dtype=np.object)
[ ]
# 各ジャンルのデータ数確認
for i, c in enumerate(categories):
    indices = np.where(train.target == i)
    print(c, len(train.data[indices]))


[ ]
# 文章の前処理
def analyzer(text):
    
    # 無視する単語
    stop_words = ['i', 'a', 'an', 'the', 'to', 'and', 'or', 'if', 'is', 'are', 'am', 'it', 'this', 'that', 'of', 'from', 'in', 'on']
    
    text = text.lower() # 小文字化
    text = text.replace('\n', '') # 改行削除
    text = text.replace('\t', '') # タブ削除
    text = re.sub(re.compile(r'[!-/:-@[-`{-~]'), ' ', text) # 記号をスペースに置き換え
…        words.append(word)
        
    return words
TF-IDF
[ ]
# TfidfVectorizerにて前処理のanalyzer関数を指定するため、そのまま渡せばOK
corpus = train.data

# モデルを定義
tfidf_vectorizer = TfidfVectorizer(analyzer=analyzer, min_df=10) # 単語が出現する文章数がmin_df未満のとき、当該単語を無視

# 定義したモデルでTF-IDFによる文章ベクトル化処理実行
tfidfs = tfidf_vectorizer.fit_transform(corpus)

tfidfs.shape


[ ]
# 語彙の確認（右の数字は登録順）
tfidf_vectorizer.vocabulary_


[ ]
# ある語彙のベクトルを確認
for i in tfidfs[0].toarray()[0]:
    print(i)


[ ]
# 高次元ベクトルを2次元に圧縮
tsne_tfidf = manifold.TSNE(n_components=2).fit_transform(tfidfs.toarray())
tsne_tfidf.shape


[ ]
# 二次元グラフでプロット
df_tsne_tfidf = pd.DataFrame({
    'x': tsne_tfidf[:, 0],
    'y': tsne_tfidf[:, 1],
    'category': train.target,
})
 
df_tsne_tfidf.plot.scatter(x='x', y='y', c='category', colormap='viridis', figsize=(7, 5), s=20)
plt.show()


Word2Vec

# Word2Vecでの文章データ入力は、予め前処理を施しておく
corpus = [analyzer(text) for text in train.data]

word2vec = Word2Vec(
    sentences=corpus,
    iter=10,  # Word2Vecニューラルネットワークのエポック数
    size=200,  # ベクトル化の次元数
    min_count=10,  # min_count未満の出現回数である単語を無視する
    window=5,  # ある単語の前後何単語を学習させるか
    sample=1e-3,  # 高頻度語を無視する確率
)

# 各文章に含まれる単語のベクトルを平均して、それを文章のベクトルとする
avg_word2vecs = np.array([
    word2vec.wv[
        list(analyzer(text) & word2vec.wv.vocab.keys())  # Word2Vec語彙に存在する文章単語に絞る
    ].mean(axis=0)  # 絞った単語のベクトル群を平均する
    for text in train.data])
avg_word2vecs.shape

(2800, 200)

# 200次元ベクトルを2次元に圧縮
tsne_avg_word2vecs = manifold.TSNE(n_components=2).fit_transform(avg_word2vecs)
tsne_avg_word2vecs.shape

(2800, 2)

# 二次元グラフでプロット
df_tsne_avg_word2vecs = pd.DataFrame({
    'x': tsne_avg_word2vecs[:, 0],
    'y': tsne_avg_word2vecs[:, 1],
    'category': train.target,
})
 
df_tsne_avg_word2vecs.plot.scatter(x='x', y='y', c='category', colormap='viridis', figsize=(7, 5), s=20)
plt.show()


[ ]
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 正規表現用
import re

# scikit-learn
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn import datasets, manifold, mixture
from sklearn.model_selection import cross_validate

# gensimの言語系ライブラリ
from gensim.models import Word2Vec
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

# Random Forest
from sklearn.ensemble import RandomForestClassifier
[ ]
# 今回扱うニュース記事のジャンル
categories = [
    'alt.atheism',  # 無神論
    'comp.graphics',  # 画像
    'rec.sport.baseball',  # 野球
    'sci.space',  # 宇宙
    'talk.politics.guns'  # 銃の所持・使用に関する政治
]

# ライブラリで用意されているデータをダウンロード
train = datasets.fetch_20newsgroups(subset='train', categories=categories)

# 分析用にデータ形式を変換
train.data = np.array(train.data, dtype=np.object)
[ ]
# 各ジャンルのデータ数確認
for i, c in enumerate(categories):
    indices = np.where(train.target == i)
    print(c, len(train.data[indices]))


[ ]
# 文章の前処理
def analyzer(text):
    
    # 無視する単語
    stop_words = ['i', 'a', 'an', 'the', 'to', 'and', 'or', 'if', 'is', 'are', 'am', 'it', 'this', 'that', 'of', 'from', 'in', 'on']
    
    text = text.lower() # 小文字化
    text = text.replace('\n', '') # 改行削除
    text = text.replace('\t', '') # タブ削除
    text = re.sub(re.compile(r'[!-/:-@[-`{-~]'), ' ', text) # 記号をスペースに置き換え
…        words.append(word)
        
    return words
TF-IDF
[ ]
# TfidfVectorizerにて前処理のanalyzer関数を指定するため、そのまま渡せばOK
corpus = train.data

# モデルを定義
tfidf_vectorizer = TfidfVectorizer(analyzer=analyzer, min_df=10) # 単語が出現する文章数がmin_df未満のとき、当該単語を無視

# 定義したモデルでTF-IDFによる文章ベクトル化処理実行
tfidfs = tfidf_vectorizer.fit_transform(corpus)

tfidfs.shape


[ ]
# 語彙の確認（右の数字は登録順）
tfidf_vectorizer.vocabulary_


[ ]
# ある語彙のベクトルを確認
for i in tfidfs[0].toarray()[0]:
    print(i)


[ ]
# 高次元ベクトルを2次元に圧縮
tsne_tfidf = manifold.TSNE(n_components=2).fit_transform(tfidfs.toarray())
tsne_tfidf.shape


[ ]
# 二次元グラフでプロット
df_tsne_tfidf = pd.DataFrame({
    'x': tsne_tfidf[:, 0],
    'y': tsne_tfidf[:, 1],
    'category': train.target,
})
 
df_tsne_tfidf.plot.scatter(x='x', y='y', c='category', colormap='viridis', figsize=(7, 5), s=20)
plt.show()


Word2Vec
[ ]
# Word2Vecでの文章データ入力は、予め前処理を施しておく
corpus = [analyzer(text) for text in train.data]

word2vec = Word2Vec(
    sentences=corpus,
    iter=10,  # Word2Vecニューラルネットワークのエポック数
    size=200,  # ベクトル化の次元数
    min_count=10,  # min_count未満の出現回数である単語を無視する
    window=5,  # ある単語の前後何単語を学習させるか
    sample=1e-3,  # 高頻度語を無視する確率
)

# 各文章に含まれる単語のベクトルを平均して、それを文章のベクトルとする
avg_word2vecs = np.array([
    word2vec.wv[
        list(analyzer(text) & word2vec.wv.vocab.keys())  # Word2Vec語彙に存在する文章単語に絞る
    ].mean(axis=0)  # 絞った単語のベクトル群を平均する
    for text in train.data])
avg_word2vecs.shape


[ ]
# 200次元ベクトルを2次元に圧縮
tsne_avg_word2vecs = manifold.TSNE(n_components=2).fit_transform(avg_word2vecs)
tsne_avg_word2vecs.shape


[ ]
# 二次元グラフでプロット
df_tsne_avg_word2vecs = pd.DataFrame({
    'x': tsne_avg_word2vecs[:, 0],
    'y': tsne_avg_word2vecs[:, 1],
    'category': train.target,
})
 
df_tsne_avg_word2vecs.plot.scatter(x='x', y='y', c='category', colormap='viridis', figsize=(7, 5), s=20)
plt.show()


Doc2Vec

# Doc2Vecでの文章データ入力は、予め前処理を施しておく
corpus = [TaggedDocument(words=analyzer(text), tags=[i]) for i, text in enumerate(train.data)]  # tagsは文章のID

doc2vec = Doc2Vec(
    documents=corpus,
    dm=1,  # モデル種類 dm == 1 -> dmpv, dm != 1 -> DBoW
    epochs=10,
    vector_size=200,
    min_count=10,
    window=5,
    sample=1e-3
)

# 各文章をベクトル化
doc2vecs = np.array([doc2vec.infer_vector(analyzer(text)) for text in train.data])
doc2vecs.shape

(2800, 200)

# 200次元ベクトルを2次元に圧縮
tsne_doc2vecs = manifold.TSNE(n_components=2).fit_transform(doc2vecs)
tsne_doc2vecs.shape

(2800, 2)

# 二次元グラフでプロット
df_tsne_doc2vecs = pd.DataFrame({
    'x': tsne_doc2vecs[:, 0],
    'y': tsne_doc2vecs[:, 1],
    'category': train.target,
})
 
df_tsne_doc2vecs.plot.scatter(x='x', y='y', c='category', colormap='viridis', figsize=(7, 5), s=20)
plt.show()


[ ]
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 正規表現用
import re

# scikit-learn
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn import datasets, manifold, mixture
from sklearn.model_selection import cross_validate

# gensimの言語系ライブラリ
from gensim.models import Word2Vec
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

# Random Forest
from sklearn.ensemble import RandomForestClassifier
[ ]
# 今回扱うニュース記事のジャンル
categories = [
    'alt.atheism',  # 無神論
    'comp.graphics',  # 画像
    'rec.sport.baseball',  # 野球
    'sci.space',  # 宇宙
    'talk.politics.guns'  # 銃の所持・使用に関する政治
]

# ライブラリで用意されているデータをダウンロード
train = datasets.fetch_20newsgroups(subset='train', categories=categories)

# 分析用にデータ形式を変換
train.data = np.array(train.data, dtype=np.object)
[ ]
# 各ジャンルのデータ数確認
for i, c in enumerate(categories):
    indices = np.where(train.target == i)
    print(c, len(train.data[indices]))


[ ]
# 文章の前処理
def analyzer(text):
    
    # 無視する単語
    stop_words = ['i', 'a', 'an', 'the', 'to', 'and', 'or', 'if', 'is', 'are', 'am', 'it', 'this', 'that', 'of', 'from', 'in', 'on']
    
    text = text.lower() # 小文字化
    text = text.replace('\n', '') # 改行削除
    text = text.replace('\t', '') # タブ削除
    text = re.sub(re.compile(r'[!-/:-@[-`{-~]'), ' ', text) # 記号をスペースに置き換え
…        words.append(word)
        
    return words
TF-IDF
[ ]
# TfidfVectorizerにて前処理のanalyzer関数を指定するため、そのまま渡せばOK
corpus = train.data

# モデルを定義
tfidf_vectorizer = TfidfVectorizer(analyzer=analyzer, min_df=10) # 単語が出現する文章数がmin_df未満のとき、当該単語を無視

# 定義したモデルでTF-IDFによる文章ベクトル化処理実行
tfidfs = tfidf_vectorizer.fit_transform(corpus)

tfidfs.shape


[ ]
# 語彙の確認（右の数字は登録順）
tfidf_vectorizer.vocabulary_


[ ]
# ある語彙のベクトルを確認
for i in tfidfs[0].toarray()[0]:
    print(i)


[ ]
# 高次元ベクトルを2次元に圧縮
tsne_tfidf = manifold.TSNE(n_components=2).fit_transform(tfidfs.toarray())
tsne_tfidf.shape


[ ]
# 二次元グラフでプロット
df_tsne_tfidf = pd.DataFrame({
    'x': tsne_tfidf[:, 0],
    'y': tsne_tfidf[:, 1],
    'category': train.target,
})
 
df_tsne_tfidf.plot.scatter(x='x', y='y', c='category', colormap='viridis', figsize=(7, 5), s=20)
plt.show()


Word2Vec
[ ]
# Word2Vecでの文章データ入力は、予め前処理を施しておく
corpus = [analyzer(text) for text in train.data]

word2vec = Word2Vec(
    sentences=corpus,
    iter=10,  # Word2Vecニューラルネットワークのエポック数
    size=200,  # ベクトル化の次元数
    min_count=10,  # min_count未満の出現回数である単語を無視する
    window=5,  # ある単語の前後何単語を学習させるか
    sample=1e-3,  # 高頻度語を無視する確率
)

# 各文章に含まれる単語のベクトルを平均して、それを文章のベクトルとする
avg_word2vecs = np.array([
    word2vec.wv[
        list(analyzer(text) & word2vec.wv.vocab.keys())  # Word2Vec語彙に存在する文章単語に絞る
    ].mean(axis=0)  # 絞った単語のベクトル群を平均する
    for text in train.data])
avg_word2vecs.shape


[ ]
# 200次元ベクトルを2次元に圧縮
tsne_avg_word2vecs = manifold.TSNE(n_components=2).fit_transform(avg_word2vecs)
tsne_avg_word2vecs.shape


[ ]
# 二次元グラフでプロット
df_tsne_avg_word2vecs = pd.DataFrame({
    'x': tsne_avg_word2vecs[:, 0],
    'y': tsne_avg_word2vecs[:, 1],
    'category': train.target,
})
 
df_tsne_avg_word2vecs.plot.scatter(x='x', y='y', c='category', colormap='viridis', figsize=(7, 5), s=20)
plt.show()


Doc2Vec
[ ]
# Doc2Vecでの文章データ入力は、予め前処理を施しておく
corpus = [TaggedDocument(words=analyzer(text), tags=[i]) for i, text in enumerate(train.data)]  # tagsは文章のID

doc2vec = Doc2Vec(
    documents=corpus,
    dm=1,  # モデル種類 dm == 1 -> dmpv, dm != 1 -> DBoW
    epochs=10,
    vector_size=200,
    min_count=10,
    window=5,
    sample=1e-3
)

# 各文章をベクトル化
doc2vecs = np.array([doc2vec.infer_vector(analyzer(text)) for text in train.data])
doc2vecs.shape


[ ]
# 200次元ベクトルを2次元に圧縮
tsne_doc2vecs = manifold.TSNE(n_components=2).fit_transform(doc2vecs)
tsne_doc2vecs.shape


[ ]
# 二次元グラフでプロット
df_tsne_doc2vecs = pd.DataFrame({
    'x': tsne_doc2vecs[:, 0],
    'y': tsne_doc2vecs[:, 1],
    'category': train.target,
})
 
df_tsne_doc2vecs.plot.scatter(x='x', y='y', c='category', colormap='viridis', figsize=(7, 5), s=20)
plt.show()


それぞれの文章ベクトル化手法によって分類モデルを構築

# 各手法のベクトル表現に対して、Random Forestを用いて分類精度を出す
model = RandomForestClassifier()

# 精度比較用
df_compare = []

# 交差検証の設定
scoring = ['accuracy']
cv_fold = 5

# tfidf
results = cross_validate(model, tfidfs.toarray(), train.target, scoring=scoring, cv=cv_fold)
df_compare.append(results["test_accuracy"])

# Word2Vec
results = cross_validate(model, avg_word2vecs, train.target, scoring=scoring, cv=cv_fold)
df_compare.append(results["test_accuracy"])

# Doc2Vec
results = cross_validate(model, doc2vecs, train.target, scoring=scoring, cv=cv_fold)
df_compare.append(results["test_accuracy"])

# プロット用にpandas形式へ変換
df_compare = pd.DataFrame(df_compare, index=['tfidf', 'Word2Vec', 'Doc2Vec']).T

# プロット
plt.figure(figsize=(12,5))
df_compare.boxplot(grid=True)
plt.title('test accuracy')
plt.show()

