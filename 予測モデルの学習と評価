# データ分割
from sklearn.model_selection import train_test_split

# Random Forestの回帰と分類
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier

# SVMの回帰(SVR: Regression)と分類(SVC: Classifier)
from sklearn.svm import SVR, SVC

# RidgeとLasso
from sklearn.linear_model import Ridge, Lasso

# Logistic回帰
from sklearn.linear_model import LogisticRegression
# データ分割
from sklearn.model_selection import train_test_split

# グリッドサーチとランダムサーチ
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# 交差検証
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict

# F値関連
from sklearn.metrics import classification_report

# R2乗値
from sklearn.metrics import r2_score

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data_wine = pd.read_csv("/content/wine (1).csv", engine = "python", encoding = "s-jis")

data_wine
	wine_type	alcohol	malic_acid	ash	alcalinity_of_ash	magnesium	total_phenols	flavanoids	nonflavanoid_phenols	proanthocyanins	color_intensity	hue	OD280/OD315_of_diluted_wines	proline
0	1	14.23	1.71	2.43	15.6	127	2.80	3.06	0.28	2.29	5.64	1.04	3.92	1065
1	1	13.20	1.78	2.14	11.2	100	2.65	2.76	0.26	1.28	4.38	1.05	3.40	1050
2	1	13.16	2.36	2.67	18.6	101	2.80	3.24	0.30	2.81	5.68	1.03	3.17	1185
3	1	14.37	1.95	2.50	16.8	113	3.85	3.49	0.24	2.18	7.80	0.86	3.45	1480
4	1	13.24	2.59	2.87	21.0	118	2.80	2.69	0.39	1.82	4.32	1.04	2.93	735
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
173	3	13.71	5.65	2.45	20.5	95	1.68	0.61	0.52	1.06	7.70	0.64	1.74	740
174	3	13.40	3.91	2.48	23.0	102	1.80	0.75	0.43	1.41	7.30	0.70	1.56	750
175	3	13.27	4.28	2.26	20.0	120	1.59	0.69	0.43	1.35	10.20	0.59	1.56	835
176	3	13.17	2.59	2.37	20.0	120	1.65	0.68	0.53	1.46	9.30	0.60	1.62	840
177	3	14.13	4.10	2.74	24.5	96	2.05	0.76	0.56	1.35	9.20	0.61	1.60	560
178 rows × 14 columns


# 説明変数xと目的変数yに分割
x = data_wine.drop("wine_type",axis=1)
y = data_wine["wine_type"]

# 学習/評価/テスト用データに分ける

# まず(テスト以外 : テスト) = (0.67 : 0.33)で分類
x_train_valid, x_test, y_train_valid, y_test = \
  train_test_split(x,y, test_size=0.33)

# 次にテスト以外をさらに(学習:評価) = (0.5:0.5)で分割
x_train, x_valid,y_train, y_valid = \
 train_test_split(x_train_valid, y_train_valid,test_size = 0.5)
 
 len(x_train)
 59
 
 len(x_valid)
 60
 
 # SVMの回帰(SVR: Regression)と分類(SVC: Classifier)
from sklearn.svm import SVR, SVC

C_list = [0.001, 0.01, 0.1, 1, 10, 100]
gamma_list = [0.001, 0.01, 0.1, 1, 10, 100]

best_score = 0
best_params = {}

for C in C_list:
  for gamma in gamma_list:
    svc = SVC(C = C, gamma = gamma)
    svc.fit(x_train, y_train)
    score = svc.score(x_valid, y_valid)

    if score > best_score:
      best_score = score
      best_params = {"C" : C, "gamma": gamma}

print("best score: {}".format(best_score))
print("best parameters: {}".format(best_params))

best score: 0.65
best parameters: {'C': 1, 'gamma': 0.001}

C_list = [0.001, 0.01, 0.1, 1, 10, 100]
gamma_list = [0.001, 0.01, 0.1, 1, 10, 100]

gscv = GridSearchCV(SVC(), {"C": C_list, "gamma":gamma_list},cv = 5)
gscv.fit(x_train_valid, y_train_valid)

print("best score: {}".format(gscv.best_score_))
print("best parameters: {}".format(gscv.best_params_))

best score: 0.7647058823529411
best parameters: {'C': 10, 'gamma': 0.001}
/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.
  DeprecationWarning)
  
  svc_best_param = SVC(**best_params)
svc_best_param.fit(x_train_valid, y_train_valid)
svc_best_param.score(x_test, y_test)

0.7288135593220338

gscv.score(x_test, y_test)

0.7627118644067796

y = data_wine["wine_type"]
x = data_wine.drop("wine_type", axis=1)

kfold = KFold(n_splits=5, shuffle=True, random_state=0)
scores = cross_val_score(svc_best_param, x, y, cv=kfold)

np.mean(scores)

0.7087301587301588

scores

array([0.72222222, 0.58333333, 0.66666667, 0.82857143, 0.74285714])

svc_no_tunig = SVC()
svc_no_tunig.fit(x_train_valid, y_train_valid)
svc_no_tunig.score(x_test, y_test)

/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.
  "avoid this warning.", FutureWarning)
0.4915254237288136

scores = cross_val_score(svc_no_tunig, x, y, cv=kfold)
np.mean(scores)

/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.
  "avoid this warning.", FutureWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.
  "avoid this warning.", FutureWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.
  "avoid this warning.", FutureWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.
  "avoid this warning.", FutureWarning)
/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.
  "avoid this warning.", FutureWarning)
0.44301587301587303

print(classification_report(y_test, svc_best_param. predict(x_test)))

             precision    recall  f1-score   support

           1       0.76      0.94      0.84        17
           2       0.80      0.64      0.71        25
           3       0.61      0.65      0.63        17

    accuracy                           0.73        59
   macro avg       0.72      0.74      0.73        59
weighted avg       0.73      0.73      0.73        59

print(classification_report(y_test, svc_no_tunig.predict(x_test)))

              precision    recall  f1-score   support

           1       1.00      0.12      0.21        17
           2       0.45      1.00      0.62        25
           3       1.00      0.12      0.21        17

    accuracy                           0.49        59
   macro avg       0.82      0.41      0.35        59
weighted avg       0.77      0.49      0.39        59

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data_house = pd.read_csv("/content/housing (1).csv", engine = "python", encoding = "s-jis")

data_house

	CRIM	ZN	INDUS	CHAS	NOX	RM	AGE	DIS	RAD	TAX	PTRATIO	B	LSTAT	MEDV
0	0.00632	18.0	2.31	0	0.538	6.575	65.2	4.0900	1	296	15.3	396.90	4.98	24.0
1	0.02731	0.0	7.07	0	0.469	6.421	78.9	4.9671	2	242	17.8	396.90	9.14	21.6
2	0.02729	0.0	7.07	0	0.469	7.185	61.1	4.9671	2	242	17.8	392.83	4.03	34.7
3	0.03237	0.0	2.18	0	0.458	6.998	45.8	6.0622	3	222	18.7	394.63	2.94	33.4
4	0.06905	0.0	2.18	0	0.458	7.147	54.2	6.0622	3	222	18.7	396.90	5.33	36.2
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
501	0.06263	0.0	11.93	0	0.573	6.593	69.1	2.4786	1	273	21.0	391.99	9.67	22.4
502	0.04527	0.0	11.93	0	0.573	6.120	76.7	2.2875	1	273	21.0	396.90	9.08	20.6
503	0.06076	0.0	11.93	0	0.573	6.976	91.0	2.1675	1	273	21.0	396.90	5.64	23.9
504	0.10959	0.0	11.93	0	0.573	6.794	89.3	2.3889	1	273	21.0	393.45	6.48	22.0
505	0.04741	0.0	11.93	0	0.573	6.030	80.8	2.5050	1	273	21.0	396.90	7.88	11.9

# SVMの回帰(SVR: Regression)と分類(SVC: Classifier)
from sklearn.svm import SVR, SVC

C_list = [0.001, 0.01, 0.1, 1, 10, 100]
gamma_list = [0.001, 0.01, 0.1, 1, 10, 100]

best_score = 0
best_params = {}

for C in C_list:
  for gamma in gamma_list:
    svc = SVC(C = C, gamma = gamma)
    svc.fit(x_train, y_train)
    score = svc.score(x_valid, y_valid)

    if score > best_score:
      best_score = score
      best_params = {"C" : C, "gamma": gamma}

print("best score: {}".format(best_score))
print("best parameters: {}".format(best_params))
